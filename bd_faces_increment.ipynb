{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNOWN FACES INCREMENT\n",
    "\n",
    "This code is used to increment \"faces\" database table with known faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "from sqlalchemy import create_engine, Column, Integer, String\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from psycopg2.errors import UniqueViolation\n",
    "import dotenv\n",
    "import os\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "import mediapipe as mp\n",
    "mp_face_mesh = mp.solutions.face_mesh\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_URL = os.getenv(\"DB_URL\")\n",
    "\n",
    "engine = create_engine(DB_URL)\n",
    "Base = declarative_base()\n",
    "Session = sessionmaker(bind=engine)\n",
    "\n",
    "class Faces(Base):\n",
    "    __tablename__ = 'faces'\n",
    "\n",
    "    id = Column(Integer, primary_key=True, index=True)\n",
    "    name = Column(String, unique=True, nullable=False)\n",
    "    embeddings = Column(String, unique=False, nullable=False)\n",
    "\n",
    "Base.metadata.create_all(bind=engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INCREMENT DATABASE WITH HISTORY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_face(frame):\n",
    "    face_mesh = mp_face_mesh.FaceMesh(\n",
    "        max_num_faces=1,\n",
    "        refine_landmarks=False,\n",
    "        min_detection_confidence=0.5,\n",
    "        min_tracking_confidence=0.5\n",
    "    )\n",
    "    with face_mesh:\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        results = face_mesh.process(frame_rgb)\n",
    "        \n",
    "        if results.multi_face_landmarks:\n",
    "            for face_landmarks in results.multi_face_landmarks:\n",
    "                h, w = frame.shape[:2]\n",
    "                x_min = w\n",
    "                y_min = h\n",
    "                x_max = y_max = 0\n",
    "                for landmark in face_landmarks.landmark:\n",
    "                    x, y = int(landmark.x * w), int(landmark.y * h)\n",
    "                    x_min = min(x_min, x)\n",
    "                    y_min = min(y_min, y)\n",
    "                    x_max = max(x_max, x)\n",
    "                    y_max = max(y_max, y)\n",
    "                return (x_min, y_min, x_max, y_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceTransformer(nn.Module):\n",
    "    def __init__(self, backbone, d_model=512, nhead=8, num_layers=3):\n",
    "        super(FaceTransformer, self).__init__()\n",
    "        self.backbone = backbone\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        self.projection = nn.Linear(512, d_model)\n",
    "        \n",
    "        self.pos_encoder = nn.Parameter(torch.randn(1, 49, d_model))\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.fc = nn.Linear(d_model * 49, 512)  # Final embedding size\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.backbone.extract_features(x)\n",
    "        features = torch.nn.functional.relu(features)\n",
    "        \n",
    "        features = features.view(features.size(0), 512, -1)\n",
    "        features = self.projection(features.permute(0, 2, 1))\n",
    "        \n",
    "        features = features + self.pos_encoder\n",
    "        \n",
    "        output = self.transformer_encoder(features.permute(1, 0, 2))\n",
    "        output = output.permute(1, 2, 0).flatten(1)\n",
    "        return self.fc(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pth_processing(fp):\n",
    "    class PreprocessInput(torch.nn.Module):\n",
    "        def forward(self, x):\n",
    "            x = x.to(torch.float32)\n",
    "            x = torch.flip(x, dims=(0,))\n",
    "            x[0, :, :] -= 91.4953\n",
    "            x[1, :, :] -= 103.8827\n",
    "            x[2, :, :] -= 131.0912\n",
    "            return x\n",
    "\n",
    "    def get_img_torch(img):\n",
    "        ttransform = transforms.Compose([\n",
    "            transforms.PILToTensor(),\n",
    "            PreprocessInput()\n",
    "        ])\n",
    "        img = img.resize((224, 224), Image.Resampling.NEAREST)\n",
    "        img = ttransform(img)\n",
    "        img = torch.unsqueeze(img, 0).to(device)\n",
    "        return img\n",
    "    return get_img_torch(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pth_backbone_model = torch.jit.load('model/torchscript_model_0_66_49_wo_gl.pth').to(device)\n",
    "model = FaceTransformer(pth_backbone_model).to(device)\n",
    "\n",
    "def extract_features(image):\n",
    "    face = detect_face(image)\n",
    "    if face is None:\n",
    "        return None\n",
    "    x1, y1, x2, y2 = face\n",
    "    face_img = Image.fromarray(cv2.cvtColor(image[y1:y2, x1:x2], cv2.COLOR_BGR2RGB))\n",
    "    face_tensor = pth_processing(face_img)\n",
    "    with torch.no_grad():\n",
    "        features = model(face_tensor)\n",
    "        \n",
    "    return features.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1723832573.489669 14892194 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88.1), renderer: Apple M1\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "W0000 00:00:1723832573.504680 14900193 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1723832573.509966 14900196 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Face with name Thiago already exists\n",
      "\n"
     ]
    }
   ],
   "source": [
    "image = cv2.imread(\"src/thiago.jpeg\")\n",
    "features = extract_features(image)\n",
    "features_string = np.array2string(features, separator=',')\n",
    "\n",
    "try:\n",
    "    with Session() as session:\n",
    "        user = Faces(name=\"Thiago\", embeddings='string')\n",
    "        \n",
    "        query_user = session.query(Faces).filter(Faces.name == user.name).first()\n",
    "        session.add(user)\n",
    "        session.commit()\n",
    "\n",
    "except Exception as e:\n",
    "    if UniqueViolation:\n",
    "        print(f\"\\nFace with name {user.name} already exists\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "facerecog",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
