{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FACE RECOGNITION\n",
    "\n",
    "This model uses continuous training, where:\n",
    "\n",
    "- a general model is used to recognize all people,\n",
    "- a specific model is used to recognize only common faces.\n",
    "\n",
    "Whenever a face achieves a similarity of 70% or higher, the specific model is retrained to improve accuracy for that face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import mediapipe as mp\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sqlalchemy import create_engine, Column, Integer, String\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "import dotenv\n",
    "import os\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DB INTEGRATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_URL = os.getenv(\"DB_URL\")\n",
    "\n",
    "engine = create_engine(DB_URL)\n",
    "Base = declarative_base()\n",
    "Session = sessionmaker(bind=engine)\n",
    "\n",
    "class Faces(Base):\n",
    "    __tablename__ = 'faces'\n",
    "    id = Column(Integer, primary_key=True, index=True)\n",
    "    name = Column(String, unique=True, nullable=False)\n",
    "    embeddings = Column(String, unique=False, nullable=False)\n",
    "\n",
    "Base.metadata.create_all(bind=engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TORCH PREPROCESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pth_processing(fp):\n",
    "    class PreprocessInput(torch.nn.Module):\n",
    "        def forward(self, x):\n",
    "            x = x.to(torch.float32)\n",
    "            x = torch.flip(x, dims=(0,))\n",
    "            x[0, :, :] -= 91.4953\n",
    "            x[1, :, :] -= 103.8827\n",
    "            x[2, :, :] -= 131.0912\n",
    "            return x\n",
    "\n",
    "    def get_img_torch(img):\n",
    "        ttransform = transforms.Compose([\n",
    "            transforms.PILToTensor(),\n",
    "            PreprocessInput()\n",
    "        ])\n",
    "        img = img.resize((224, 224), Image.Resampling.NEAREST)\n",
    "        img = ttransform(img)\n",
    "        img = torch.unsqueeze(img, 0).to(device)\n",
    "        return img\n",
    "    return get_img_torch(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRANSFORMER MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceTransformer(nn.Module):\n",
    "    def __init__(self, backbone, d_model=512, nhead=8, num_layers=3):\n",
    "        super(FaceTransformer, self).__init__()\n",
    "        self.backbone = backbone\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # Projection layer to adjust feature dimensions\n",
    "        self.projection = nn.Linear(512, d_model)\n",
    "        \n",
    "        self.pos_encoder = nn.Parameter(torch.randn(1, 49, d_model))\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.fc = nn.Linear(d_model * 49, 512)  # Final embedding size\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.backbone.extract_features(x)\n",
    "        features = torch.nn.functional.relu(features)\n",
    "        \n",
    "        # Reshape and project features\n",
    "        features = features.view(features.size(0), 512, -1)\n",
    "        features = self.projection(features.permute(0, 2, 1))\n",
    "        \n",
    "        # Add positional encoding\n",
    "        features = features + self.pos_encoder\n",
    "        \n",
    "        # Pass through transformer\n",
    "        output = self.transformer_encoder(features.permute(1, 0, 2))\n",
    "        output = output.permute(1, 2, 0).flatten(1)\n",
    "        return self.fc(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MEDIAPIPE - DETECT FACE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_face(frame):\n",
    "    face_mesh = mp_face_mesh.FaceMesh(\n",
    "        max_num_faces=1,\n",
    "        refine_landmarks=False,\n",
    "        min_detection_confidence=0.5,\n",
    "        min_tracking_confidence=0.5\n",
    "    )\n",
    "    with face_mesh:\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        results = face_mesh.process(frame_rgb)\n",
    "        \n",
    "        if results.multi_face_landmarks:\n",
    "            for face_landmarks in results.multi_face_landmarks:\n",
    "                h, w = frame.shape[:2]\n",
    "                x_min = w\n",
    "                y_min = h\n",
    "                x_max = y_max = 0\n",
    "                for landmark in face_landmarks.landmark:\n",
    "                    x, y = int(landmark.x * w), int(landmark.y * h)\n",
    "                    x_min = min(x_min, x)\n",
    "                    y_min = min(y_min, y)\n",
    "                    x_max = max(x_max, x)\n",
    "                    y_max = max(y_max, y)\n",
    "                return (x_min, y_min, x_max, y_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXTRACT FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "pth_backbone_model = torch.jit.load('model/torchscript_model_0_66_49_wo_gl.pth').to(device)\n",
    "model = FaceTransformer(pth_backbone_model).to(device)\n",
    "\n",
    "def extract_features(image):\n",
    "    face = detect_face(image)\n",
    "    if face is None:\n",
    "        return None\n",
    "    x1, y1, x2, y2 = face\n",
    "    face_img = Image.fromarray(cv2.cvtColor(image[y1:y2, x1:x2], cv2.COLOR_BGR2RGB))\n",
    "    face_tensor = pth_processing(face_img)\n",
    "    with torch.no_grad():\n",
    "        features = model(face_tensor)\n",
    "        \n",
    "    return features.cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### COMPARE FACES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_faces(img1, img2):\n",
    "    features1 = extract_features(img1)\n",
    "    features2 = extract_features(img2)\n",
    "    if features1 is None or features2 is None:\n",
    "        return None\n",
    "    similarity = cosine_similarity(features1, features2)[0][0]\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Face similarity: 0.8058371543884277\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1723824533.420526 14719781 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88.1), renderer: Apple M1\n",
      "W0000 00:00:1723824533.421757 14746633 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1723824533.423873 14746633 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1723824533.438512 14719781 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88.1), renderer: Apple M1\n",
      "W0000 00:00:1723824533.439593 14746644 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1723824533.441646 14746645 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1723824533.454790 14719781 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88.1), renderer: Apple M1\n",
      "W0000 00:00:1723824533.455871 14746649 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1723824533.457599 14746649 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "I0000 00:00:1723824533.545102 14719781 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88.1), renderer: Apple M1\n",
      "W0000 00:00:1723824533.546044 14746659 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1723824533.547895 14746657 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    }
   ],
   "source": [
    "image1 = cv2.imread(\"src/photo1.jpeg\")\n",
    "image2 = cv2.imread(\"src/photo2.jpeg\")\n",
    "face1 = detect_face(image1)\n",
    "face2 = detect_face(image2)\n",
    "similarity = compare_faces(image1, image2)\n",
    "\n",
    "print(f\"\\n\\nFace similarity: {similarity}\\n\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cvision",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
