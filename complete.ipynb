{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMPLETE CODE\n",
    "\n",
    "This code has the function of mantaining everything, all functions at the same place with the purpose of testing new functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine, Column, Integer, String\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from psycopg2.errors import UniqueViolation\n",
    "\n",
    "import json\n",
    "import dotenv\n",
    "import os\n",
    "dotenv.load_dotenv()\n",
    "DB_URL = os.getenv(\"DB_URL\")\n",
    "\n",
    "engine = create_engine(DB_URL)\n",
    "Base = declarative_base()\n",
    "Session = sessionmaker(bind=engine)\n",
    "\n",
    "class Faces(Base):\n",
    "    __tablename__ = 'faces'\n",
    "    id = Column(Integer, primary_key=True, index=True)\n",
    "    name = Column(String, unique=True, nullable=False)\n",
    "    embeddings = Column(String, unique=False, nullable=False)\n",
    "    \n",
    "Base.metadata.create_all(bind=engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CREATE EMBEDDINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings(name, embeddings):\n",
    "    try:\n",
    "        with Session() as session:\n",
    "            embeddings = json.dumps(embeddings.tolist())\n",
    "            user = Faces(name=name, embeddings=embeddings)\n",
    "            session.add(user)\n",
    "            session.commit()\n",
    "\n",
    "    except Exception as e:\n",
    "        if UniqueViolation:\n",
    "            print(f\"\\nFace with name {user.name} already exists\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "READ EMBEDDINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_embeddings(name):\n",
    "    try:\n",
    "        with Session() as session:\n",
    "            user = session.query(Faces).filter(Faces.name == name).first()\n",
    "            if user:\n",
    "                embeddings = np.array(json.loads(user.embeddings), dtype=np.float32)\n",
    "                return embeddings\n",
    "            else:\n",
    "                return f'No face with name {name} found'\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError getting embedding: {e}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UPDATE EMBEDDINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_embeddings(name, embeddings):\n",
    "    try:\n",
    "        with Session() as session:\n",
    "            user = session.query(Faces).filter(Faces.name == name).first()\n",
    "            if user:\n",
    "                user.embeddings = json.dumps(embeddings.tolist())\n",
    "                session.commit()\n",
    "            else:\n",
    "                return Exception(\"User not found\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError getting embedding: {e}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DETECT FACE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "\n",
    "def detect_face(frame):\n",
    "    face_mesh = mp_face_mesh.FaceMesh(\n",
    "        max_num_faces=1,\n",
    "        refine_landmarks=False,\n",
    "        min_detection_confidence=0.5,\n",
    "        min_tracking_confidence=0.5\n",
    "    )\n",
    "    \n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = face_mesh.process(frame_rgb)\n",
    "    \n",
    "    if results.multi_face_landmarks:\n",
    "        for face_landmarks in results.multi_face_landmarks:\n",
    "            h, w = frame.shape[:2]\n",
    "            x_min = w\n",
    "            y_min = h\n",
    "            x_max = y_max = 0\n",
    "            for landmark in face_landmarks.landmark:\n",
    "                x, y = int(landmark.x * w), int(landmark.y * h)\n",
    "                x_min = min(x_min, x)\n",
    "                y_min = min(y_min, y)\n",
    "                x_max = max(x_max, x)\n",
    "                y_max = max(y_max, y)\n",
    "            return (x_min, y_min, x_max, y_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PTH PROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "def pth_processing(fp):\n",
    "    class PreprocessInput(nn.Module):\n",
    "        def forward(self, x):\n",
    "            x = x.to(torch.float32)\n",
    "            x = torch.flip(x, dims=(0,))\n",
    "            x[0, :, :] -= 91.4953\n",
    "            x[1, :, :] -= 103.8827\n",
    "            x[2, :, :] -= 131.0912\n",
    "            return x\n",
    "\n",
    "    def get_img_torch(img):\n",
    "        ttransform = transforms.Compose([\n",
    "            transforms.PILToTensor(),\n",
    "            PreprocessInput()\n",
    "        ])\n",
    "        img = img.resize((224, 224), Image.Resampling.NEAREST)\n",
    "        img = ttransform(img)\n",
    "        img = torch.unsqueeze(img, 0).to(device)\n",
    "        return img\n",
    "    return get_img_torch(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FACE TRANSFORMER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceTransformer(nn.Module):\n",
    "    def __init__(self, backbone, d_model=512, nhead=8, num_layers=3):\n",
    "        super(FaceTransformer, self).__init__()\n",
    "        self.backbone = backbone\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        self.projection = nn.Linear(512, d_model)\n",
    "        \n",
    "        self.pos_encoder = nn.Parameter(torch.randn(1, 49, d_model))\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.fc = nn.Linear(d_model * 49, 512)  # Final embedding size\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.backbone.extract_features(x)\n",
    "        features = torch.nn.functional.relu(features)\n",
    "        \n",
    "        features = features.view(features.size(0), 512, -1)\n",
    "        features = self.projection(features.permute(0, 2, 1))\n",
    "        \n",
    "        features = features + self.pos_encoder\n",
    "        \n",
    "        output = self.transformer_encoder(features.permute(1, 0, 2))\n",
    "        output = output.permute(1, 2, 0).flatten(1)\n",
    "        return self.fc(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EXTRACT FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pth_backbone_model = torch.jit.load('model/torchscript_model_0_66_49_wo_gl.pth').to(device)\n",
    "model = FaceTransformer(pth_backbone_model).to(device)\n",
    "\n",
    "def extract_features(image):\n",
    "    \"\"\"\n",
    "        Extract features from a cv2 image.\n",
    "        The code itself detects the face in the image and extracts the features.\n",
    "    \"\"\"\n",
    "    face = detect_face(image)\n",
    "    if face is None:\n",
    "        return None\n",
    "    x1, y1, x2, y2 = face\n",
    "    face_img = Image.fromarray(cv2.cvtColor(image[y1:y2, x1:x2], cv2.COLOR_BGR2RGB))\n",
    "    face_tensor = pth_processing(face_img)\n",
    "    with torch.no_grad():\n",
    "        features = model(face_tensor)\n",
    "        \n",
    "    return features.cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "COMPARE EMBEDDINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def compare_embeddings(features1, features2):\n",
    "    \"\"\"\n",
    "        Compare the similarity between two features.\n",
    "        The similarity is calculated using cosine similarity.\n",
    "        Use extract_features to get the features before comparing.\n",
    "    \"\"\"\n",
    "    if features1 is None or features2 is None:\n",
    "        return None\n",
    "    similarity = cosine_similarity(features1, features2)[0][0]\n",
    "    \n",
    "    return similarity"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "facerecog",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
